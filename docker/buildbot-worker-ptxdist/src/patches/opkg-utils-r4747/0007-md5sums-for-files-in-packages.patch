From: "Joseph A. Lutz" <joseph.lutz@novatechweb.com>
Date: Thu, 22 Mar 2012 13:30:52 -0500
Subject: [PATCH] md5sums for files in packages

This patch adds the python programs to generate the file the test
station needs to check that all the files install have the correct
checksums.

The python program uses the arpy module.

Signed-off-by: Joseph A. Lutz <joseph.lutz@novatechweb.com>
---
 Makefile                  |   2 +-
 arpy.py                   | 295 ++++++++++++++++++++++++++++++++++++++++++++++
 process_distfiles_md5sums | 234 ++++++++++++++++++++++++++++++++++++
 3 files changed, 530 insertions(+), 1 deletion(-)
 create mode 100644 arpy.py
 create mode 100755 process_distfiles_md5sums

diff --git a/Makefile b/Makefile
index e92dbb3..764d17d 100644
--- a/Makefile
+++ b/Makefile
@@ -1,6 +1,6 @@
 UTILS = opkg-build opkg-unbuild opkg-compare-versions opkg-make-index opkg.py \
         opkg-list-fields arfile.py opkg-buildpackage opkg-diff opkg-extract-file opkg-show-deps \
-        opkg-compare-indexes opkg-compare-versions.sh
+        opkg-compare-indexes opkg-compare-versions.sh arpy.py process_distfiles_md5sums
 
 DESTDIR=
 PREFIX=/usr/local
diff --git a/arpy.py b/arpy.py
new file mode 100644
index 0000000..613d323
--- /dev/null
+++ b/arpy.py
@@ -0,0 +1,295 @@
+# -*- coding: utf-8 -*-
+#
+# Copyright 2011 Stanisław Pitucha. All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without modification, are
+# permitted provided that the following conditions are met:
+#
+#    1. Redistributions of source code must retain the above copyright notice, this list of
+#       conditions and the following disclaimer.
+#
+#    2. Redistributions in binary form must reproduce the above copyright notice, this list
+#       of conditions and the following disclaimer in the documentation and/or other materials
+#       provided with the distribution.
+#
+# THIS SOFTWARE IS PROVIDED BY Stanisław Pitucha ``AS IS'' AND ANY EXPRESS OR IMPLIED
+# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
+# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL Stanisław Pitucha OR
+# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
+# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+# ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+# The views and conclusions contained in the software and documentation are those of the
+# authors and should not be interpreted as representing official policies, either expressed
+# or implied, of Stanisław Pitucha.
+#
+
+"""
+arpy module can be used for reading `ar` files' headers, as well as accessing
+the data contained in the archive. Archived files are accessible via file-like
+objects.
+Support for both GNU and BSD extended length filenames is included.
+
+In order to read the file, create a new proxy with:
+ar = arpy.Archive('some_ar_file')
+ar.read_all_headers()
+
+The list of file names can be listed through:
+ar.archived_files.keys()
+
+Files themselves can be opened by getting the value of:
+f = ar.archived_files['filename']
+
+and read through:
+f.read([length])
+
+random access through seek and tell functions is supported on the archived files
+"""
+
+HEADER_BSD = 1
+HEADER_GNU = 2
+HEADER_GNU_TABLE = 3
+HEADER_GNU_SYMBOLS = 4
+HEADER_NORMAL = 5
+HEADER_TYPES = {
+		HEADER_BSD: 'BSD',
+		HEADER_GNU: 'GNU', HEADER_GNU_TABLE: 'GNU_TABLE',
+		HEADER_GNU_SYMBOLS: 'GNU_SYMBOLS',
+		HEADER_NORMAL: 'NORMAL'}
+
+GLOBAL_HEADER_LEN = 8
+HEADER_LEN = 60
+
+class ArchiveFormatError(Exception):
+	""" Raised on problems with parsing the archive headers """
+	pass
+class ArchiveAccessError(IOError):
+	""" Raised on problems with accessing the archived files """
+	pass
+
+class ArchiveFileHeader(object):
+	""" File header of an archived file, or a special data segment """
+
+	def __init__(self, header, offset):
+		""" Creates a new header from binary data starting at a specified offset """
+		import struct
+		
+		name, timestamp, uid, gid, mode, size, magic = struct.unpack(
+				"16s 12s 6s 6s 8s 10s 2s", header)
+		if magic != "\x60\x0a":
+			raise ArchiveFormatError("file header magic doesn't match")
+
+		if name.startswith("#1/"):
+			self.type = HEADER_BSD
+		elif name.startswith("//"):
+			self.type = HEADER_GNU_TABLE
+		elif name.strip() == "/":
+			self.type = HEADER_GNU_SYMBOLS
+		elif name.startswith("/"):
+			self.type = HEADER_GNU
+		else:
+			self.type = HEADER_NORMAL
+
+		try:
+			self.size = int(size)
+
+			if self.type in (HEADER_NORMAL, HEADER_BSD, HEADER_GNU):
+				self.timestamp = int(timestamp)
+				self.uid = int(uid)
+				self.gid = int(gid)
+				self.mode = int(mode, 8)
+
+		except ValueError, err:
+			raise ArchiveFormatError(
+					"cannot convert file header fields to integers", err)
+
+		self.offset = offset
+		name = name.rstrip()
+		if len(name) > 1:
+			name = name.rstrip('/')
+
+		if self.type == HEADER_NORMAL:
+			self.name = name
+			self.file_offset = offset + HEADER_LEN
+		else:
+			self.name = None
+			self.proxy_name = name
+			self.file_offset = None
+
+	def __repr__(self):
+		""" Creates a human-readable summary of a header """
+		if self.type in (HEADER_NORMAL, HEADER_BSD, HEADER_GNU):
+			return '''<ArchiveFileHeader: "%s" type:%s size:%i>''' % (self.name,
+					HEADER_TYPES[self.type], self.size)
+		else:
+			return '''<ArchiveFileHeader: "%s" type:%s>''' % (self.name,
+					HEADER_TYPES.get(self.type, "unknown %i" % (self.type,)))
+
+class ArchiveFileData(object):
+	""" File-like object used for reading an archived file """
+
+	def __init__(self, file_obj, header):
+		"""
+		Creates a new proxy for the archived file, reusing the archive's file descriptor
+		"""
+		self.header = header
+		self.file = file_obj
+		self.last_offset = 0
+
+	def read(self, size = None):
+		""" Reads the data from the archived file, simulates file.read """
+		if size is None:
+			size = self.header.size
+
+		if self.header.size < self.last_offset + size:
+			size = self.header.size - self.last_offset
+
+		self.file.seek(self.header.file_offset + self.last_offset)
+		data = self.file.read(size)
+		if len(data) < size:
+			raise ArchiveAccessError("incorrect archive file")
+
+		self.last_offset += size
+		return data
+
+	def tell(self):
+		""" Returns the position in archived file, simulates file.tell """
+		return self.last_offset
+
+	def seek(self, offset, whence = 0):
+		""" Sets the position in archived file, simulates file.seek """
+		if whence == 0:
+			pass # absolute
+		elif whence == 1:
+			offset += self.last_offset
+		elif whence == 2:
+			offset += self.header.size
+		else:
+			raise ArchiveAccessError("invalid argument")
+		
+		if offset < 0 or offset > self.header.size:
+			raise ArchiveAccessError("incorrect file position")
+		self.last_offset = offset
+
+class Archive(object):
+	""" Archive object allowing reading of *.ar files """
+	def __init__(self, filename):
+		self.headers = []
+		self.filename = filename
+		self.file = open(self.filename, "rb")
+		if self.file.read(GLOBAL_HEADER_LEN) != "!<arch>\n":
+			raise ArchiveFormatError("file is missing the global header")
+		
+		self.next_header_offset = GLOBAL_HEADER_LEN
+		self.gnu_table = None
+		self.archived_files = {}
+
+	def __read_file_header(self, offset = None):
+		""" Reads and returns a single new file header """
+		if offset is not None:
+			self.file.seek(offset)
+		else:
+			offset = self.file.tell()
+
+		header = self.file.read(HEADER_LEN)
+
+		if len(header) == 0:
+			return None
+		if len(header) < HEADER_LEN:
+			raise ArchiveFormatError("file header too short")
+		
+		file_header = ArchiveFileHeader(header, offset)
+		if file_header.type == HEADER_GNU_TABLE:
+			self.__read_gnu_table(file_header.size)
+			
+		add_len = self.__fix_name(file_header)
+		file_header.file_offset = offset + HEADER_LEN + add_len
+
+		if offset == self.next_header_offset:
+			new_offset = offset + HEADER_LEN + add_len + file_header.size
+			self.next_header_offset = Archive.__pad2(new_offset)
+
+		return file_header
+
+	def __read_gnu_table(self, size):
+		""" Reads the table of filenames specific to GNU ar format """
+		table_string = self.file.read(size)
+		if len(table_string) != size:
+			raise ArchiveFormatError("file too short to fit the names table")
+
+		self.gnu_table = {}
+		
+		position = 0
+		for filename in table_string.split("\n"):
+			self.gnu_table[position] = filename
+			position += len(filename) + 1
+
+	def __fix_name(self, header):
+		"""
+		Corrects the long filename using the format-specific method.
+		That means either looking up the name in GNU filename table, or
+		reading past the header in BSD ar files.
+		"""
+		if header.type == HEADER_NORMAL:
+			pass
+
+		elif header.type == HEADER_BSD:
+			filename_len = Archive.__get_bsd_filename_len(header.name)
+			self.file.seek(header.offset + HEADER_LEN)
+			header.name = self.file.read(filename_len)
+			return filename_len
+
+		elif header.type == HEADER_GNU_TABLE:
+			header.name = "*GNU_TABLE*"
+
+		elif header.type == HEADER_GNU:
+			gnu_position = int(header.proxy_name[1:])
+			if gnu_position not in self.gnu_table:
+				raise ArchiveFormatError("file references a name not present in the index")
+			header.name = self.gnu_table[gnu_position]
+			
+		elif header.type == HEADER_GNU_SYMBOLS:
+			pass
+
+		else:
+			raise NotImplementedError("strange header, not implemented yet")
+		
+		return 0
+
+	@staticmethod
+	def __pad2(num):
+		""" Returns a 2-aligned offset """
+		if num % 2 == 0:
+			return num
+		else:
+			return num+1
+
+	@staticmethod
+	def __get_bsd_filename_len(name):
+		""" Returns the length of the filename for a BSD style header """
+		filename_len = name[3:]
+		return int(filename_len)
+
+	def read_next_header(self):
+		"""
+		Reads a single new header, returning a its representation, or None at the end of file
+		"""
+		header = self.__read_file_header(self.next_header_offset)
+		if header is not None:
+			self.headers.append(header)
+			if header.type in (HEADER_BSD, HEADER_NORMAL, HEADER_GNU):
+				self.archived_files[header.name] = ArchiveFileData(self.file, header)
+
+		return header
+
+	def read_all_headers(self):
+		""" Reads all headers """
+		while self.read_next_header() is not None:
+			pass
+
+	def close(self):
+		""" Closes the archive file descriptor """
+		self.file.close()
diff --git a/process_distfiles_md5sums b/process_distfiles_md5sums
new file mode 100755
index 0000000..8a3536e
--- /dev/null
+++ b/process_distfiles_md5sums
@@ -0,0 +1,234 @@
+#! /usr/bin/env python2
+import random
+import multiprocessing
+import time
+import os
+import sys
+import arpy
+import tarfile
+import re
+import csv
+import string
+import hashlib
+
+DEBUG=False
+# DEBUG=True
+
+EMPTY_FILE_CHECKSUM = hashlib.md5('').hexdigest()
+
+def process_control( control_data, t_file, f_name ):
+	for t_member in t_file.getmembers():
+		if t_member.name in ['./control']:
+			file_obj = t_file.extractfile( t_member )
+			line = file_obj.readline()
+			while True:
+				if not line: break
+				lineparts = re.match(r'([\w-]*?):\s*(.*)', string.rstrip(line))
+				if lineparts:
+					name = string.lower(lineparts.group(1))
+					value = lineparts.group(2)
+					# Check if the value continues to the next line
+					while True:
+						line = file_obj.readline()
+						if not line: break
+						if line[0] != ' ': break
+						line = string.rstrip(line)
+						value = value + '\n' + line
+					# Store the values of the controll file feilds
+					if control_data.has_key(name):
+						control_data[name] = value
+				else:
+					line = file_obj.readline()
+		if t_member.name in ['./conffiles']:
+			# read the file and assume each line is a config file
+			file_obj = t_file.extractfile( t_member )
+			for line in file_obj.readlines():
+				# add the normalized config file to the list of config files
+				control_data['conffiles'].append( os.path.normpath(os.path.join('/', line.rstrip())) )
+
+def process_data( control_data, t_file ):
+	CHUNKSIZE = 512
+	return_result = []
+	
+	for t_member in filter( lambda f: f.isfile() or f.islnk(), t_file.getmembers() ):
+		# Normalize the path name so I can check if it matches a config file
+		conffiles = os.path.normpath(os.path.join('/', t_member.name)) in control_data['conffiles']
+		if DEBUG:
+			print "    file (%1d) : \"%s\"" % (conffiles, t_member.name),
+		# Calculate the checksum for the file member
+		checksum = hashlib.md5()
+		if t_member.isfile() or t_member.islnk():
+			if DEBUG:
+				print "  \t[file/link]"
+			f_member = t_file.extractfile( t_member )
+			try:
+				data_buffer = f_member.read(CHUNKSIZE)
+				while data_buffer:
+					checksum.update( data_buffer )
+					data_buffer = f_member.read(CHUNKSIZE)
+			except:
+				print "ERROR:  exception reached while calculating checksum for : %s" % (t_member.name)
+			finally:
+				f_member.close()
+			# store the collected information
+			return_result.append( (control_data['package'], t_member.name, conffiles, checksum.hexdigest()) )
+		else:
+			if DEBUG:
+				print ""
+		#elif t_member.isdir():
+		#	return_result.append( (control_data['package'], t_member.name, conffiles, "[          DIRECTORY           ]") )
+		# let the garbage collecter collect this memory
+		f_member = None
+		data_buffer = None
+		checksum = None
+	if DEBUG and (len(return_result) is 0):
+		print "  No files to get chechsum upon."
+	return return_result
+
+def process_ipk( file_name ):
+	assert file_name!=None
+	assert os.path.exists( file_name )
+	
+	control_data = {
+		'package': None,
+		'architecture': None,
+		'conffiles': []
+		}
+	
+	# open the ar archive and read the headers
+	ar = arpy.Archive( file_name )
+	ar.read_all_headers()
+	# Read the control file and gather the information from them
+	if ar.archived_files.has_key('control.tar.gz'):
+		process_control( control_data, tarfile.open( fileobj=ar.archived_files['control.tar.gz'] ), file_name )
+	else:
+		print "ERROR:  Could not find the 'control.tar.gz' file inside the ar archive."
+		return [['None', './', 'False', EMPTY_FILE_CHECKSUM],]
+	# Verify the package name exists
+	if control_data['package'] is None:
+		print "ERROR:  Can not determine package name from file: %s" % file_name
+		return [['None', './', 'False', EMPTY_FILE_CHECKSUM],]
+	# Examine the data of the files and gather the checksums
+	if ar.archived_files.has_key('data.tar.gz'):
+		data = process_data( control_data, tarfile.open( fileobj=ar.archived_files['data.tar.gz'] ) )
+	else:
+		print "ERROR:  Could not find the 'data.tar.gz' file inside the ar archive."
+		return [['None', './', 'False', EMPTY_FILE_CHECKSUM],]
+	if DEBUG:
+		print "  len(data) : %d" % len(data)
+	if len(data) is 0:
+		print "Warning!: Package without files to calculate cheacksum on."
+		return [[control_data['package'], './', 'False', EMPTY_FILE_CHECKSUM],]
+	# Return the data as an array to be inserted into the CSV file
+	return data
+
+def main( argv ):
+	try:	# How many processes do we want to run on this machine
+		num_processes = multiprocessing.cpu_count()
+	except  NotImplementedError:
+		num_processes = 2
+	if not DEBUG:
+		# create a pool of processes to use
+		process_pool = multiprocessing.Pool( processes=num_processes )
+	
+	# Get the list of the packages
+	dist_package_files = map(
+		lambda f: os.path.join( argv['directory'], f ),
+		filter(
+			lambda f: f.endswith( '.ipk' ),
+			os.listdir( argv['directory'] ) ) )
+	
+	if not DEBUG:
+		# get a list of work items and the size of the chunk
+		chunk_size = int( ( len(dist_package_files) / num_processes )/2 )
+	
+	# start the work and write the result to a file
+	md5sumFile = open(argv['output_file'], "wb")
+	md5sumFile.write('# Version 2\n')
+	csvWriter = csv.writer( md5sumFile )
+	csvWriter.writerow(['Package', 'Path', 'ConfFile', 'Hash'])
+	if DEBUG:
+		for package_file in dist_package_files:
+			print "Processing \"%s\"" % (package_file)
+			for line in process_ipk(package_file):
+				csvWriter.writerow(line)
+	else:
+		for result in process_pool.map_async(process_ipk, dist_package_files, chunk_size).get():
+			for line in result:
+				csvWriter.writerow(line)
+	md5sumFile.close()
+	
+	if not DEBUG:
+		process_pool.close()
+		process_pool.join()
+
+def usage():
+	import sys
+	sys.stderr.write("%s -R <Repository Directory> -P <Project Name> -D <Distribution> -F <Output Filename>\n" % (sys.argv[0],))
+	sys.stderr.write("\n")
+	sys.stderr.write("  %s Creates a comma seperated file for a particular distribution. This file is used on\n" % (sys.argv[0],))
+	sys.stderr.write("  our test station to verify the files were written to the drive correctly and the unit is ready to be shipped.\n")
+	sys.stderr.write("\n")
+	sys.stderr.write("    -R : Specifies the path to where the repositories are located.\n")
+	sys.stderr.write("    -P : Specifies the name of the project.\n")
+	sys.stderr.write("    -D : Specifies the Distribution to be processed.\n")
+	sys.stderr.write("    -F : Specifies the output file that will be created. (usualy: Packages.file_md5sum.csv)\n")
+	sys.exit(-1)
+
+# REPO_PATH = '/srv/www/repositories/'
+# REPO_NAME = 'OrionLX-armeb-xscale-glibc'
+# REPO_NAME = 'OrionLX-i686-glibc'
+# REPO_VERSION = '7.2.2'
+def arguments():
+	import sys
+	arg = {}
+	arg['repo_path'] = None
+	arg['repo_name'] = None
+	arg['distribution'] = None
+	arg['output_filename'] = None
+	count = 1
+	while count < len(sys.argv):
+		if sys.argv[count] in ['-R', '-P', '-D', '-F']:
+			if count+1 >= len(sys.argv):
+				sys.stderr.write("A value is needed for this argument : %s\n" % (sys.argv[count]))
+				usage()
+			if sys.argv[count] in ['-R']:
+				if not os.path.exists(sys.argv[count+1]) or not os.path.isdir(sys.argv[count+1]):
+					sys.stderr.write("Directory does not exist : %s\n" % (sys.argv[count+1]))
+					usage()
+				arg['repo_path'] = sys.argv[count+1]
+			elif sys.argv[count] in ['-P']:
+				arg['repo_name'] = sys.argv[count+1]
+			elif sys.argv[count] in ['-D']:
+				arg['distribution'] = sys.argv[count+1]
+			elif sys.argv[count] in ['-F']:
+				arg['output_filename'] = sys.argv[count+1]
+			count += 1
+		else:
+			sys.stderr.write("Invalid argument : %s\n" % (sys.argv[count]))
+			usage()
+		count += 1
+	if (	arg['repo_path'] is None or
+			arg['repo_name'] is None or
+			arg['distribution'] is None or
+			arg['output_filename'] is None	):
+		usage()
+	return arg
+
+if __name__ == "__main__":
+	# Get arguments to execute the program
+	arg = arguments()
+	
+	# assemble the data variables to use based on the commandline values
+	data = {}
+	data['directory'] = os.path.join( arg['repo_path'], arg['repo_name'], 'dists', arg['distribution'] )
+	if not os.path.exists(data['directory']) or not os.path.isdir(data['directory']):
+		print "Directory does not exist: %s" % data['directory']
+		sys.exit(-1)
+	data['output_file'] = os.path.join( data['directory'], arg['output_filename'] )
+	if os.path.exists(data['output_file']):
+		os.unlink(data['output_file'])
+	
+	print "Creating file : %s" % (data['output_file'])
+	
+	main( data )
